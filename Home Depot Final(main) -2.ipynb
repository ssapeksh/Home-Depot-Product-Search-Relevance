{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SAPEKSHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pickle\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost\n",
    "from rank_bm25 import BM25\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge,Lasso,ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data (74067, 5)\n",
      "Attribute_data (2044803, 3)\n",
      "description_data (124428, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = zipfile.ZipFile('G:/Applied_AI/case_study_1/train.csv.zip')\n",
    "train_data = pd.read_csv(train_data.open('train.csv'),encoding = \"ISO-8859-1\")\n",
    "print('train_data',train_data.shape)\n",
    "\n",
    "attribute_data = zipfile.ZipFile('G:/Applied_AI/case_study_1/attributes.csv.zip')\n",
    "attribute_data = pd.read_csv(attribute_data.open('attributes.csv'),encoding = \"ISO-8859-1\")\n",
    "print('Attribute_data',attribute_data.shape)\n",
    "\n",
    "description_data = zipfile.ZipFile('G:/Applied_AI/case_study_1/product_descriptions.csv.zip')\n",
    "description_data = pd.read_csv(description_data.open('product_descriptions.csv'),encoding = \"ISO-8859-1\")\n",
    "print('description_data',description_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_attributes(df):\n",
    "    attr = attribute_data.copy()\n",
    "    product_uid = df['product_uid'].values\n",
    "    \n",
    "    temp = attr.loc[attr['product_uid'].isin(product_uid)] \n",
    "    temp['combine_feature'] = temp['name'] + ' ' + temp['value']\n",
    "    \n",
    "    brands = temp[temp['name']=='MFG Brand Name']\n",
    "    brands['brand'] = brands['value']\n",
    "    brands.drop(['name','value','combine_feature'],axis=1,inplace=True)\n",
    "\n",
    "    temp= temp.merge(brands,on='product_uid',how='left')\n",
    "    temp['combine_feature_'] = temp.groupby('product_uid')['combine_feature'].transform(lambda x :''.join(str(x)))\n",
    "    temp = temp.drop_duplicates(subset=['product_uid'])\n",
    "    df = df.merge(temp,on='product_uid',how='left').set_index(df.index)\n",
    "    df.drop(['name','value','combine_feature'],axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def merge_description(df):\n",
    "    descrip = description_data.copy()\n",
    "    product_uid = df['product_uid'].values\n",
    "    temp = descrip.loc[descrip['product_uid'].isin(product_uid)]\n",
    "    df = df.merge(temp,on='product_uid',how='left').set_index(df.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_n_words(n,text):\n",
    "    if n>len(text.split()):\n",
    "        return 'invalid'\n",
    "    return ' '.join(text.split()[:n])\n",
    "\n",
    "def fill_brand(df):\n",
    "    null_brand_values = df[df['brand'].isna()]\n",
    "    unique_brands = df['brand'].unique()\n",
    "\n",
    "    for i,j in null_brand_values.iterrows():\n",
    "        title=j['product_title']\n",
    "        if extract_n_words(6,title) in unique_brands:\n",
    "            null_brand_values['brand'].loc[i] = extract_n_words(6, title)\n",
    "        elif extract_n_words(5,title) in unique_brands:\n",
    "            null_brand_values['brand'].loc[i] = extract_n_words(5, title)\n",
    "        elif extract_n_words(4,title) in unique_brands:\n",
    "            null_brand_values['brand'].loc[i] = extract_n_words(4, title)\n",
    "        elif extract_n_words(3,title) in unique_brands:\n",
    "            null_brand_values['brand'].loc[i] = extract_n_words(3, title)\n",
    "        elif extract_n_words(2,title) in unique_brands:\n",
    "            null_brand_values['brand'].loc[i] = extract_n_words(2, title)\n",
    "        else:\n",
    "            null_brand_values['brand'].loc[i] = extract_n_words(1, title)\n",
    "            \n",
    "    df['brand'].loc[null_brand_values.index]=null_brand_values['brand'].values\n",
    "    return df\n",
    "\n",
    "def fill_attributes(df):\n",
    "    null_df = df[df['combine_feature_'].isna()]\n",
    "    null_df['combine_feature_'] = null_df['product_description'].copy()\n",
    "    df['combine_feature_'].loc[null_df.index] = null_df['combine_feature_'].values\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#Reference : https://towardsdatascience.com/modeling-product-search-relevance-in-e-commerce-home-depot-case-study-8ccb56fbc5ab\n",
    "\n",
    "def standardize_units(text):\n",
    "    text = \" \" + text + \" \"\n",
    "    text = re.sub('( gal | gals | galon )',' gallon ',text)\n",
    "    text = re.sub('( ft | fts | feets | foot | foots )',' feet ',text)\n",
    "    text = re.sub('( squares | sq )',' square ',text)\n",
    "    text = re.sub('( lb | lbs | pounds )',' pound ',text)\n",
    "    text = re.sub('( oz | ozs | ounces | ounc )',' ounce ',text)\n",
    "    text = re.sub('( yds | yd | yards )',' yard ',text)\n",
    "    return text\n",
    "\n",
    "def preprocessing(text):\n",
    "    \n",
    "    text = text.replace('in.','inch')  # Replace in. with inch\n",
    "    text = re.sub('[^A-Za-z0-9.]+',' ',text) # remove special characters except '.'\n",
    "    text = re.sub(r\"(?<!\\d)[.,;:](?!\\d)\",'',text,0) # https://stackoverflow.com/questions/43142710/remove-all-punctuation-from-string-except-if-its-between-digits\n",
    "    text = re.sub(\"[A-Za-z]+\", lambda ele: \" \" + ele[0] + \" \", text)\n",
    "    text = standardize_units(text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stopwords_stemming(text):\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words] # Stopwords\n",
    "    words = [ps.stem(word) for word in words] # stemming\n",
    "    return ' '.join(words)\n",
    "\n",
    "def stemming_search(text):\n",
    "    words = text.split()\n",
    "    words = [ps.stem(word) for word in words] # stemming\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def common_word(feature_1,feature_2):\n",
    "    common_word=[]\n",
    "    words,count = feature_1.split(),0\n",
    "    for i in words:\n",
    "        set_1 = set(feature_1.split())\n",
    "        set_2 = set(feature_2.split())\n",
    "        common_ = set_1.intersection(set_2)\n",
    "        common_ = ' '.join(common_)\n",
    "        common_word.append(common_)\n",
    "        return ''.join([i for i in common_word])\n",
    "    \n",
    "def cosine_similarity(feature_1,feature_2):\n",
    "    f_1 = set(feature_1.split())\n",
    "    f_2 = set(feature_2.split())\n",
    "    num = len(f_1.intersection(f_2))\n",
    "    deno = np.sqrt(len(f_1)) * np.sqrt(len(f_2))\n",
    "    \n",
    "    if deno == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return num/deno\n",
    "    \n",
    "def jaccard_coefficient(feature_1,feature_2):\n",
    "    f_1 = set(feature_1.split())\n",
    "    f_2 = set(feature_2.split())\n",
    "    num = len(f_1.intersection(f_2))\n",
    "    deno = len(f_1 | f_2)\n",
    "    if deno == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return num/deno\n",
    "    \n",
    "from rank_bm25 import BM25\n",
    "\n",
    "def bm25_params(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(corpus)\n",
    "    idf_name_value = dict(zip(vectorizer.get_feature_names(),(list(vectorizer.idf_))))\n",
    "    length = [len(i.split()) for i in corpus]\n",
    "    avgdl = np.average(length)                      \n",
    "    param = {'idf_name_value':idf_name_value,'avgdl':avgdl,'len_corpus':len(corpus)}\n",
    "    return param\n",
    "\n",
    "def bm25_scores(param,text,query,k=1.5,b=0.75):\n",
    "    idf_name_value = param['idf_name_value']\n",
    "    avgdl = param['avgdl']\n",
    "    N=param['len_corpus']\n",
    "    score = 0\n",
    "    \n",
    "    for word in query.split():\n",
    "        mod_d = len(text.split())  # len of document\n",
    "        n_tf = text.count(word)   # no of times query occur in document\n",
    "        \n",
    "        if word in idf_name_value.keys():  # check if word present in document\n",
    "            idf_score = idf_name_value[word]\n",
    "        else:\n",
    "            idf_score = np.log(1+N)+1    #  idf for words not in document    \n",
    "        score_ = idf_score * (n_tf*(k+1) / (n_tf + k * (1-b + b * (mod_d / avgdl))))\n",
    "        score+=score_\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('G:/Final Data_1/dataset_title_brand_descrip.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "def corrected_terms(text):\n",
    "    temp = text.split()\n",
    "    temp = [correction(word) for word in temp]\n",
    "    return ' '.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G:/Final Data_1/Features/title.pkl','rb') as f:\n",
    "    vectorizer_title = pickle.load(f)\n",
    "\n",
    "with open('G:/Final Data_1/Features/search.pkl','rb') as f:\n",
    "    vectorizer_search = pickle.load(f)\n",
    "\n",
    "with open('G:/Final Data_1/Features/brand.pkl','rb') as f:\n",
    "    vectorizer_brand = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/description.pkl','rb') as f:\n",
    "    vectorizer_des = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/combine_feature_.pkl','rb') as f:\n",
    "    vectorizer_combine_ft = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/common_word_ST.pkl','rb') as f:\n",
    "    vectorizer_common_ST = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/common_word_SD.pkl','rb') as f:\n",
    "    vectorizer_common_SD = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/common_word_SB.pkl','rb') as f:\n",
    "    vectorizer_common_SB = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/len_product_title.pkl','rb') as f:\n",
    "    normalizer_ft1 = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/len_search_term.pkl','rb') as f:\n",
    "    normalizer_ft2 = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/len_of_brand.pkl','rb') as f:\n",
    "    normalizer_ft3 = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/len_product_description.pkl','rb') as f:\n",
    "    normalizer_ft4 = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/len_combine_feature_.pkl','rb') as f:\n",
    "    normalizer_ft5 = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/num_common_word_SB.pkl','rb') as f:\n",
    "    normalizer_ft6 = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/Features/num_common_word_SD.pkl','rb') as f:\n",
    "    normalizer_ft7 = pickle.load(f)\n",
    "\n",
    "    \n",
    "with open('G:/Final Data_1/Features/num_common_word_ST.pkl','rb') as f:\n",
    "    normalizer_ft8 = pickle.load(f)\n",
    "\n",
    "    \n",
    "with open('G:/Final Data_1/Features/BM25_ST.pkl','rb') as f:\n",
    "    normalizer_ft9 = pickle.load(f)\n",
    "\n",
    "with open('G:/Final Data_1/Features/BM25_SB.pkl','rb') as f:\n",
    "    normalizer_ft10 = pickle.load(f)\n",
    "\n",
    "with open('G:/Final Data_1/Features/BM25_SD.pkl','rb') as f:\n",
    "    normalizer_ft11 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('G:/Final Data_1/Best_Model/GBDT.pkl','rb') as f:\n",
    "    model_gbdt = pickle.load(f)\n",
    "    \n",
    "with open('G:/Final Data_1/BM25_model.pkl','rb') as f:\n",
    "    bm25_model = pickle.load(f)\n",
    "    \n",
    "dataset = pd.read_pickle('G:/Final Data_1/Final_Database_bm25.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>brand</th>\n",
       "      <th>combine_feature_</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>simpson strong tie 12 gaug angl</td>\n",
       "      <td>angl bracket</td>\n",
       "      <td>simpson strong tie</td>\n",
       "      <td>0 bullet 01 versatil connector variou 90 1 bul...</td>\n",
       "      <td>angl make joint stronger also provid consist s...</td>\n",
       "      <td>simpson strong tie 12 gaug angl simpson strong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>behr premium textur deckov 1 gallon sc 141 tug...</td>\n",
       "      <td>deck over</td>\n",
       "      <td>behr premium textur deckov</td>\n",
       "      <td>15 applic method brush roller spray 16 assembl...</td>\n",
       "      <td>behr premium textur deckov innov solid color c...</td>\n",
       "      <td>behr premium textur deckov 1 gallon sc 141 tug...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid                                      product_title  \\\n",
       "0       100001                    simpson strong tie 12 gaug angl   \n",
       "2       100002  behr premium textur deckov 1 gallon sc 141 tug...   \n",
       "\n",
       "    search_term                       brand  \\\n",
       "0  angl bracket          simpson strong tie   \n",
       "2     deck over  behr premium textur deckov   \n",
       "\n",
       "                                    combine_feature_  \\\n",
       "0  0 bullet 01 versatil connector variou 90 1 bul...   \n",
       "2  15 applic method brush roller spray 16 assembl...   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  angl make joint stronger also provid consist s...   \n",
       "2  behr premium textur deckov innov solid color c...   \n",
       "\n",
       "                                        product_info  \n",
       "0  simpson strong tie 12 gaug angl simpson strong...  \n",
       "2  behr premium textur deckov 1 gallon sc 141 tug...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_1 = pd.read_pickle('G:/Final Data_1/clean_test_df.pkl')\n",
    "test_data = train_data.loc[temp_1.index[:]]\n",
    "true_labels = train_data.loc[temp_1.index[:]]['relevance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71395</th>\n",
       "      <td>213805</td>\n",
       "      <td>200118</td>\n",
       "      <td>BEHR Premium Plus Ultra 8 oz. #BIC-05 Shabby C...</td>\n",
       "      <td>shabby pink</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73433</th>\n",
       "      <td>219609</td>\n",
       "      <td>205038</td>\n",
       "      <td>GROHE Grandera 2-Handle Low Arc Vessel Valve T...</td>\n",
       "      <td>vesel tub</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38810</th>\n",
       "      <td>118575</td>\n",
       "      <td>139844</td>\n",
       "      <td>Deck and Floor Sander Finishing Pad 4-1/2 in. ...</td>\n",
       "      <td>deck pad</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  product_uid                                      product_title  \\\n",
       "71395  213805       200118  BEHR Premium Plus Ultra 8 oz. #BIC-05 Shabby C...   \n",
       "73433  219609       205038  GROHE Grandera 2-Handle Low Arc Vessel Valve T...   \n",
       "38810  118575       139844  Deck and Floor Sander Finishing Pad 4-1/2 in. ...   \n",
       "\n",
       "       search_term  relevance  \n",
       "71395  shabby pink        3.0  \n",
       "73433    vesel tub        3.0  \n",
       "38810     deck pad        3.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Data_1 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final(data_input):\n",
    "    temp = data_input.copy()\n",
    "    dataset = merge_attributes(temp)\n",
    "    dataset = merge_description(dataset)\n",
    "    dataset = fill_brand(dataset)\n",
    "    dataset = fill_attributes(dataset)\n",
    "\n",
    "    data = dataset.copy()\n",
    "\n",
    "    data['product_title'] = data['product_title'].apply(lambda x: preprocessing(x))\n",
    "    data['search_term'] = data['search_term'].apply(lambda x: preprocessing(x)) \n",
    "    data['brand'] = data['brand'].apply(lambda x: preprocessing(x))\n",
    "    data['combine_feature_'] = data['combine_feature_'].apply(lambda x: preprocessing(x))\n",
    "    data['product_description'] =data['product_description'].apply(lambda x: preprocessing(x))\n",
    "\n",
    "    \"\"\"\n",
    "    furthur preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    data['product_title'] = data['product_title'].apply(lambda x: stopwords_stemming(x))\n",
    "    data['search_term'] = data['search_term'].apply(lambda x: stemming_search(x))\n",
    "    data['brand'] = data['brand'].apply(lambda x: stopwords_stemming(x))\n",
    "    data['combine_feature_'] = data['combine_feature_'].apply(lambda x: stopwords_stemming(x))\n",
    "    data['product_description'] = data['product_description'].apply(lambda x: stopwords_stemming(x))\n",
    "\n",
    " \n",
    "    data['product_info'] = data['product_title'] + ' \\t ' + data['product_description'] + ' \\t ' + data['brand']+ ' \\t ' + data['search_term']\n",
    "\n",
    "\n",
    "    data['len_product_title'] = data['product_title'].str.split().apply(len)\n",
    "    data['len_search_term'] = data['search_term'].str.split().apply(len)\n",
    "    data['len_of_brand'] = data['brand'].str.split().apply(len)\n",
    "    data['len_product_description'] = data['product_description'].str.split().apply(len)\n",
    "    data['len_combine_feature_'] = data['combine_feature_'].str.split().apply(len)\n",
    "\n",
    "\n",
    "    data['common_word_ST'] = data['product_info'].map(lambda x: common_word(x.split('\\t')[3],x.split('\\t')[0]))\n",
    "    data['common_word_SD'] = data['product_info'].map(lambda x: common_word(x.split('\\t')[3],x.split('\\t')[1]))\n",
    "    data['common_word_SB'] = data['product_info'].map(lambda x: common_word(x.split('\\t')[3],x.split('\\t')[2]))\n",
    "\n",
    "\n",
    "    # num common words\n",
    "    data['num_common_word_ST'] = data['common_word_ST'].apply(lambda x: len(x.split()))\n",
    "    data['num_common_word_SD'] = data['common_word_SD'].apply(lambda x: len(x.split()))\n",
    "    data['num_common_word_SB'] = data['common_word_SB'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # cosine distance\n",
    "    data['cosine_ST'] = data.apply(lambda x: cosine_similarity(x['search_term'],x['product_title']),axis=1)\n",
    "    data['cosine_SB'] = data.apply(lambda x: cosine_similarity(x['search_term'],x['brand']),axis=1)\n",
    "    data['cosine_SD'] = data.apply(lambda x: cosine_similarity(x['search_term'],x['product_description']),axis=1)\n",
    "\n",
    "    # Jaccard Coefficient\n",
    "    data['jaccard_ST'] = data.apply(lambda x: jaccard_coefficient(x['search_term'],x['product_title']),axis=1)\n",
    "    data['jaccard_SB'] = data.apply(lambda x: jaccard_coefficient(x['search_term'],x['brand']),axis=1)\n",
    "    data['jaccard_SD'] = data.apply(lambda x: jaccard_coefficient(x['search_term'],x['product_description']),axis=1)\n",
    "\n",
    "    data['ratio_title_search'] = data['len_product_title'] / data['len_search_term']\n",
    "    data['ratio_descrip_search'] = data['len_product_description'] / data['len_search_term']\n",
    "    data['ratio_common_ST_to_search_term'] = data['num_common_word_ST'] / data['len_search_term']\n",
    "    data['ratio_common_SD_to_search_term'] = data['num_common_word_SD'] / data['len_search_term']\n",
    "    data['ratio_common_SB_to_search_term'] = data['num_common_word_SB'] / data['len_search_term']\n",
    "\n",
    "\n",
    "    #---------------------------------search to title------------------------------------\n",
    "    param_title = bm25_params(data['product_title'])\n",
    "    data['BM25_ST']  = data.apply(lambda x : bm25_scores(param_title,x['product_title'],x['search_term']),axis=1)\n",
    "    #---------------------------------search to brand------------------------------------\n",
    "    param_brand = bm25_params(data['brand'])\n",
    "    data['BM25_SB']  = data.apply(lambda x : bm25_scores(param_brand,x['brand'],x['search_term']),axis=1)\n",
    "    #---------------------------------search to description------------------------------------\n",
    "    param_desc = bm25_params(data['product_description'])\n",
    "    data['BM25_SD']  = data.apply(lambda x : bm25_scores(param_brand,x['product_description'],x['search_term']),axis=1)\n",
    "\n",
    "    title =vectorizer_title.transform(data['product_title'].values)\n",
    "    search =vectorizer_search.transform(data['search_term'].values)\n",
    "    brand =vectorizer_brand.transform(data['brand'].values)\n",
    "    des =vectorizer_des.transform(data['product_description'].values)\n",
    "    combine_ft =vectorizer_combine_ft.transform(data['combine_feature_'].values)\n",
    "\n",
    "    common_ST =vectorizer_common_ST.transform(data['common_word_ST'].values)\n",
    "    common_SD =vectorizer_common_SD.transform(data['common_word_SD'].values)\n",
    "    common_SB =vectorizer_common_SB.transform(data['common_word_SB'].values)\n",
    "\n",
    "    len_title =normalizer_ft1.transform(data[\"len_product_title\"].values.reshape(-1,1))\n",
    "    len_search_term =normalizer_ft2.transform(data[\"len_search_term\"].values.reshape(-1,1))\n",
    "    len_brand =normalizer_ft3.transform(data[\"len_of_brand\"].values.reshape(-1,1))\n",
    "    len_des =normalizer_ft4.transform(data[\"len_product_description\"].values.reshape(-1,1))\n",
    "    len_combine_ft =normalizer_ft5.transform(data[\"len_combine_feature_\"].values.reshape(-1,1))\n",
    "\n",
    "    num_SB =normalizer_ft6.transform(data[\"num_common_word_SB\"].values.reshape(-1,1))\n",
    "    num_SD =normalizer_ft7.transform(data[\"num_common_word_SD\"].values.reshape(-1,1))\n",
    "    num_ST =normalizer_ft8.transform(data[\"num_common_word_ST\"].values.reshape(-1,1))\n",
    "\n",
    "    bm25_st =normalizer_ft9.transform(data[\"BM25_ST\"].values.reshape(-1,1))\n",
    "    bm25_sb =normalizer_ft10.transform(data[\"BM25_SB\"].values.reshape(-1,1))\n",
    "    bm25_sd =normalizer_ft11.transform(data[\"BM25_SD\"].values.reshape(-1,1))\n",
    "\n",
    "    cosine_ST = data['cosine_ST'].values.reshape(-1,1)\n",
    "    cosine_SD = data['cosine_SD'].values.reshape(-1,1)\n",
    "    cosine_SB = data['cosine_SB'].values.reshape(-1,1)\n",
    "\n",
    "    jaccard_ST = data['jaccard_ST'].values.reshape(-1,1)\n",
    "    jaccard_SD = data['jaccard_SD'].values.reshape(-1,1)\n",
    "    jaccard_SB = data['jaccard_SB'].values.reshape(-1,1)\n",
    "\n",
    "    ratio_title_search = data['ratio_title_search'].values.reshape(-1,1)\n",
    "    ratio_desc_search = data['ratio_descrip_search'].values.reshape(-1,1)\n",
    "    ratio_common_ST_search = data['ratio_common_ST_to_search_term'].values.reshape(-1,1)\n",
    "    ratio_common_SD_search = data['ratio_common_SD_to_search_term'].values.reshape(-1,1)\n",
    "    ratio_common_SB_search = data['ratio_common_SB_to_search_term'].values.reshape(-1,1)\n",
    "\n",
    "    stack_data=hstack((title,search,des,brand,combine_ft,len_brand,len_combine_ft,len_des,len_search_term,len_title,\n",
    "                      common_SB,common_SD,common_ST,num_SB,num_SD,num_ST,cosine_SB,cosine_SD,cosine_ST,jaccard_SB,jaccard_SD,\n",
    "                      jaccard_ST,ratio_common_SB_search,ratio_common_SD_search,ratio_common_ST_search,\n",
    "                      ratio_desc_search,ratio_title_search,bm25_sb,bm25_sd,bm25_st)).tocsr()\n",
    "    \n",
    "    \n",
    "    predict = model_gbdt.predict(stack_data)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict 16401 products is 324.0 seconds\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "prediction = final(test_data)\n",
    "end = time.time()\n",
    "\n",
    "print('Time taken to predict {0} products is {1} seconds'.format(len(prediction),np.round(end-start),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4592174382726836"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(prediction,true_labels,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate relevant results to search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dataset['product_info'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(search,N):\n",
    "    corrected_search_query = corrected_terms(search)\n",
    "    tokenized_query = corrected_search_query.split(' ')\n",
    "    data = bm25_model.get_top_n(documents=corpus , query=tokenized_query,n=N)\n",
    "    data_result = dataset[dataset['product_info'].isin(data)]\n",
    "    data_result['search_term'] = corrected_search_query\n",
    "    features =['product_uid','product_title','search_term']\n",
    "    return data_result[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(search,N):\n",
    "    test = get_data(search,N)\n",
    "    test['relevance'] =final(test)\n",
    "    return test.sort_values('relevance',ascending=False)['product_title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant products to the search \"water heater\" are:\n",
      "1 watt galvan steel water heater earthquak strap kit\n",
      "2 everbilt 30 inch plastic water heater drain pan\n",
      "3 everbilt r valu 9 water heater blanket\n",
      "4 stainless steel water heater strap\n",
      "5 everbilt electr water heater tune kit\n",
      "6 holdrit 22 inch aluminum water heater pan box 6\n",
      "7 everbilt 24 inch x 24 inch x 72.5 inch galvan steel water heater enclosur\n",
      "8 holdrit 26 inch aluminum water heater pan box 6\n",
      "9 fiberglass water heater insul blanket\n",
      "10 eastman 22 inch x 24 inch plastic water heater pan drain fit black\n",
      "Time taken: 0.843 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "search = 'water heater'\n",
    "print('The most relevant products to the search \"{}\" are:'.format(search))\n",
    "for i, prod in enumerate(main(search, 10)):\n",
    "    print(i+1, prod)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', round(end-start,3), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant products to the search \"woter heater\" are:\n",
      "1 watt galvan steel water heater earthquak strap kit\n",
      "2 everbilt 30 inch plastic water heater drain pan\n",
      "3 everbilt r valu 9 water heater blanket\n",
      "4 stainless steel water heater strap\n",
      "5 everbilt electr water heater tune kit\n",
      "6 holdrit 22 inch aluminum water heater pan box 6\n",
      "7 everbilt 24 inch x 24 inch x 72.5 inch galvan steel water heater enclosur\n",
      "8 holdrit 26 inch aluminum water heater pan box 6\n",
      "9 fiberglass water heater insul blanket\n",
      "10 eastman 22 inch x 24 inch plastic water heater pan drain fit black\n",
      "Time taken: 0.617 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "search = 'woter heater'\n",
    "print('The most relevant products to the search \"{}\" are:'.format(search))\n",
    "for i, prod in enumerate(main(search, 10)):\n",
    "    print(i+1, prod)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', round(end-start,3), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant products to the search \"cooler\" are:\n",
      "1 ao cooler 30 qt canva cooler shoulder strap wide outsid pocket\n",
      "2 countri cooler 54 qt texa longhorn cooler\n",
      "3 dial 10 inch evapor cooler pump pigtail receptacl\n",
      "4 weatherguard 37 inch x 37 inch x 37 inch evapor cooler draft cover\n",
      "5 evapor cooler cover tie strap 2 pack\n",
      "6 weatherguard 37 inch x 37 inch x 45 inch evapor cooler draft cover\n",
      "7 weatherguard 28 inch x 28 inch x 34 inch evapor cooler draft cover\n",
      "8 weatherguard 46 inch x 46 inch x 26 inch evapor cooler draft cover\n",
      "9 weatherguard 40 inch x 33 inch round evapor cooler draft cover\n",
      "10 weatherguard 40 inch x 40 inch x 45 inch evapor cooler draft canva cover\n",
      "Time taken: 0.534 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "search = 'cooler'\n",
    "print('The most relevant products to the search \"{}\" are:'.format(search))\n",
    "for i, prod in enumerate(main(search, 10)):\n",
    "    print(i+1, prod)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', round(end-start,3), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant products to the search \"knif\" are:\n",
      "1 dewalt fix blade util knife\n",
      "2 dewalt autoload util knife pocket knife combo 2 piec\n",
      "3 huski 2.4 inch compact retract util knife\n",
      "4 huski 2.4 inch quick relea retract util knife\n",
      "5 wal board tool 8 inch tape knife\n",
      "6 huski 2.4 inch twin blade fold util knife\n",
      "7 huski 4.5 inch fold retract lock back util knife\n",
      "8 oster granger 14 piec cutleri set\n",
      "9 oster huxford 14 piec cutleri set\n",
      "10 oster baldwyn 14 piec cutleri set\n",
      "Time taken: 0.677 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "search = 'knif'\n",
    "print('The most relevant products to the search \"{}\" are:'.format(search))\n",
    "for i, prod in enumerate(main(search, 10)):\n",
    "    print(i+1, prod)\n",
    "\n",
    "end = time.time()\n",
    "print('Time taken:', round(end-start,3), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
